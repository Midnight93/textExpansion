{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from model import *\n",
    "from util import *\n",
    "from config import USE_CUDA\n",
    "import sys\n",
    "import os\n",
    "from config import MAX_LENGTH, USE_CUDA, teacher_forcing_ratio, save_dir\n",
    "from masked_cross_entropy import *\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from load import SOS_token, EOS_token, PAD_token, UNK_token\n",
    "from model import EncoderRNN, LuongAttnDecoderRNN, AttributeEncoder, Attn, AttributeAttn\n",
    "import pickle\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    " \n",
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "        # return mean of sentence_score\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topi = topi.squeeze(0)\n",
    "        topv = topv.squeeze(0)\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.idx2word[idx] for idx in self.sentence_idxes] + ['<eos>'], \n",
    "                                   self.avgScore())) # tuple(word_list, score_float) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] # pass by value\n",
    "            scores = self.sentence_scores[:] # pass by value\n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<eos>')\n",
    "            else:\n",
    "                words.append(voc.idx2word[self.sentence_idxes[i]])\n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<eos>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_out1, encoder_out2, encoder_out3, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for t in range(max_length):\n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = Variable(torch.LongTensor([[sentence.last_idx]]))\n",
    "            decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input\n",
    "\n",
    "            decoder_output, decoder_hidden, attn1, attn2, attn3, gate = decoder(decoder_input, decoder_hidden, encoder_out1, encoder_out2, encoder_out3, encoder_out4)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "\n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_layers, hidden_size, reverse, modelFile, beam_size, corpus = 2, 512, False, \"/data2/jianmo/amazon/Electronics/expansion/model/2_512/9_lexicon_title_expansion_model.tar\" \\\n",
    ", 1, \"Electronics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading training data ...\n"
     ]
    }
   ],
   "source": [
    "voc, pairs, valid_pairs, test_pairs = loadPrepareData(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n"
     ]
    }
   ],
   "source": [
    "print('Building encoder and decoder ...')\n",
    "# aspect\n",
    "with open(os.path.join(save_dir, '15_aspect.pkl'), 'rb') as fp:\n",
    "    aspect_ids = pickle.load(fp)\n",
    "aspect_num = 15 # 15 | 20 main aspects and each of them has 100 words\n",
    "aspect_ids = Variable(torch.LongTensor(aspect_ids), requires_grad=False) # convert list into torch Variable, used to index word embedding\n",
    "# attribute embeddings\n",
    "attr_size = 64 # \n",
    "attr_num = 2\n",
    "\n",
    "with open(os.path.join(save_dir, 'user_item.pkl'), 'rb') as fp:\n",
    "    user_dict, item_dict = pickle.load(fp)\n",
    "num_user = len(user_dict)\n",
    "num_item = len(item_dict)\n",
    "attr_embeddings = []\n",
    "attr_embeddings.append(nn.Embedding(num_user, attr_size))\n",
    "attr_embeddings.append(nn.Embedding(num_item, attr_size))\n",
    "aspect_embeddings = []\n",
    "aspect_embeddings.append(nn.Embedding(num_user, aspect_num))\n",
    "aspect_embeddings.append(nn.Embedding(num_item, aspect_num))\n",
    "if USE_CUDA:\n",
    "    for attr_embedding in attr_embeddings:\n",
    "        attr_embedding = attr_embedding.cuda()\n",
    "    for aspect_embedding in aspect_embeddings:\n",
    "        aspect_embedding = aspect_embedding.cuda()\n",
    "    aspect_ids = aspect_ids.cuda()\n",
    "\n",
    "encoder1 = AttributeEncoder(attr_size, attr_num, hidden_size, attr_embeddings, n_layers)\n",
    "encoder2 = AttributeEncoder(aspect_num, attr_num, hidden_size, aspect_embeddings, n_layers)\n",
    "embedding = nn.Embedding(voc.n_words, hidden_size)\n",
    "encoder3 = EncoderRNN(voc.n_words, hidden_size, embedding, n_layers)\n",
    "\n",
    "attn_model = 'dot'\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, attr_size, voc.n_words, aspect_ids, n_layers)\n",
    "checkpoint = torch.load(modelFile)\n",
    "encoder1.load_state_dict(checkpoint['en1'])\n",
    "encoder2.load_state_dict(checkpoint['en2'])\n",
    "encoder3.load_state_dict(checkpoint['en3'])\n",
    "decoder.load_state_dict(checkpoint['de'])\n",
    "\n",
    "# use cuda\n",
    "if USE_CUDA:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    encoder2 = encoder2.cuda()\n",
    "    encoder3 = encoder3.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "# train mode set to false, effect only on dropout, batchNorm\n",
    "encoder1.train(False);\n",
    "encoder2.train(False);\n",
    "encoder3.train(False);\n",
    "decoder.train(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = []\n",
    "for pair in test_pairs:\n",
    "    u = pair[0][0]\n",
    "    if u == 70451:\n",
    "        cands.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70451, 45320, ['easy', 'to', 'use', 'and', 'nice', 'standard', 'apps', '.'], ['samsung', 'galaxy', 'tab', '2', '(', '10.1-inch', ',', 'wi-fi', ')', '2012', 'model']], ['<str>', 'the', 'display', 'is', 'beautiful', 'and', 'the', 'tablet', 'is', 'very', 'easy', 'to', 'use', '.', 'it', 'comes', 'with', 'some', 'really', 'nice', 'standard', 'apps', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(cands[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair, beam_size, max_length = cands[0][0], 1, MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = pair[:2] # (user_id, item_id)\n",
    "attr_input = Variable(torch.LongTensor([sentence]), volatile=True)\n",
    "attr_input = attr_input.cuda() if USE_CUDA else attr_input\n",
    "\n",
    "sentence = pair[2] # summary\n",
    "indexes_batch = [indexesFromSentence(voc, sentence)] #[1, seq_len]\n",
    "summary_input_lengths = [len(indexes) for indexes in indexes_batch]\n",
    "summary_input = Variable(torch.LongTensor(indexes_batch), volatile=True).transpose(0, 1)\n",
    "summary_input = summary_input.cuda() if USE_CUDA else input_batch\n",
    "\n",
    "sentence = pair[3] # title\n",
    "indexes_batch = [indexesFromSentence(voc, sentence)] #[1, seq_len]\n",
    "title_input_lengths = [len(indexes) for indexes in indexes_batch]\n",
    "title_input = Variable(torch.LongTensor(indexes_batch), volatile=True).transpose(0, 1)\n",
    "title_input = title_input.cuda() if USE_CUDA else input_batch\n",
    "\n",
    "encoder_out1, encoder_out2, encoder_hidden = encoder3(summary_input, summary_input_lengths, title_input, title_input_lengths, None) # summary encoder\n",
    "encoder_out3, encoder1_hidden = encoder1(attr_input) # attribute encoder\n",
    "encoder_out4, encoder2_hidden = encoder2(attr_input) # aspect encoder\n",
    "\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers] + encoder1_hidden[:decoder.n_layers] + encoder2_hidden[:decoder.n_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input\n",
    "\n",
    "decoded_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn1s = []\n",
    "attn2s = []\n",
    "attn3s = []\n",
    "attn4s = []\n",
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di in range(max_length):\n",
    "    decoder_output, decoder_hidden, attn1, attn2, attn3, gate = decoder(decoder_input, decoder_hidden, encoder_out1, encoder_out2, encoder_out3, encoder_out4)\n",
    "    \n",
    "    attn1s.append(attn1)\n",
    "    attn2s.append(attn2)\n",
    "    attn3s.append(attn3)\n",
    "    attn4s.append(gate)\n",
    "    outputs.append(decoder_output.data)\n",
    "    topv, topi = decoder_output.data.topk(3)\n",
    "    topi = topi.squeeze(0)\n",
    "    topv = topv.squeeze(0)\n",
    "    ni = topi[0][0]\n",
    "    if ni == EOS_token:\n",
    "        decoded_words.append('<eos>')\n",
    "        break\n",
    "    else:\n",
    "        decoded_words.append(voc.idx2word[ni])\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "    decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'tablet', '.', 'it', 'is', 'easy', 'to', 'use', 'and', 'the', 'screen', 'is', 'very', 'responsive', '.', 'i', 'love', 'the', 'fact', 'that', 'it', 'has', 'a', 'micro', 'sd', 'slot', '.', 'i', 'have', 'no', 'complaints', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " (0 ,.,.) = \n",
       "   9.8674  9.0436  8.6628\n",
       " [torch.cuda.FloatTensor of size 1x1x3 (GPU 0)], \n",
       " (0 ,.,.) = \n",
       "    131   959  1032\n",
       " [torch.cuda.LongTensor of size 1x1x3 (GPU 0)])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[12].topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'screen'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2word[131]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.9999  0.9786  0.9999 -0.9976  0.9996  0.1774 -0.9981  0.9965 -0.9996\n",
       "\n",
       "Columns 9 to 14 \n",
       "   0.9594  0.9985 -0.9996  1.0000 -1.0000  0.9964\n",
       "[torch.cuda.FloatTensor of size 1x1x15 (GPU 0)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn4s[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.2056  0.1267  0.1529  0.0957  0.1474  0.1326  0.0787  0.0605\n",
       "[torch.cuda.FloatTensor of size 1x1x8 (GPU 0)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1s[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.1823  0.1599  0.1670  0.1297  0.0855  0.0874  0.0472  0.0550  0.0453\n",
       "\n",
       "Columns 9 to 10 \n",
       "   0.0258  0.0148\n",
       "[torch.cuda.FloatTensor of size 1x1x11 (GPU 0)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn2s[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.4686  0.5314\n",
       "[torch.cuda.FloatTensor of size 1x1x2 (GPU 0)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn3s[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
